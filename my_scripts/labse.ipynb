{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labse预训练模型下载地址：https://huggingface.co/sentence-transformers/LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /share/jinchang/miniconda3/envs/llama2/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /share/jinchang/miniconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n",
      "[2023-09-20 14:05:16,590] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid option string 'path of filted file': must start with a character '-'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--output-path\u001b[39m\u001b[39m\"\u001b[39m,help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpath of the output lines\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--output-filter-path\u001b[39m\u001b[39m\"\u001b[39m,help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpath of the output file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m parser\u001b[39m.\u001b[39;49madd_argument(\u001b[39m\"\u001b[39;49m\u001b[39m--output-filter-path\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mpath of filted file\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     48\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--embed-only\u001b[39m\u001b[39m\"\u001b[39m,action\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstore_true\u001b[39m\u001b[39m\"\u001b[39m,help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monly claculate embed of source senteces\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--f-content-prefix\u001b[39m\u001b[39m\"\u001b[39m,help\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath of file content stored in a json file\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/share/jinchang/miniconda3/envs/llama2/lib/python3.9/argparse.py:1409\u001b[0m, in \u001b[0;36m_ActionsContainer.add_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_positional_kwargs(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1407\u001b[0m \u001b[39m# otherwise, we're adding an optional argument\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1409\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_optional_kwargs(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1411\u001b[0m \u001b[39m# if no default was supplied, use the parser-level default\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/share/jinchang/miniconda3/envs/llama2/lib/python3.9/argparse.py:1544\u001b[0m, in \u001b[0;36m_ActionsContainer._get_optional_kwargs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39moption\u001b[39m\u001b[39m'\u001b[39m: option_string,\n\u001b[1;32m   1541\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mprefix_chars\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefix_chars}\n\u001b[1;32m   1542\u001b[0m     msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39minvalid option string \u001b[39m\u001b[39m%(option)r\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1543\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mmust start with a character \u001b[39m\u001b[39m%(prefix_chars)r\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg \u001b[39m%\u001b[39m args)\n\u001b[1;32m   1546\u001b[0m \u001b[39m# strings starting with two prefix characters are long options\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m option_strings\u001b[39m.\u001b[39mappend(option_string)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid option string 'path of filted file': must start with a character '-'"
     ]
    }
   ],
   "source": [
    "## 下面是alignment_extractor.py文件\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import json as js\n",
    "import pdb\n",
    "\n",
    "\n",
    "def read_in_chunks(file_path):\n",
    "\tfile_object = open(file_path)\n",
    "\twhile True:\n",
    "\t\tline = file_object.readline()\n",
    "\t\tif not line:\n",
    "\t\t\tbreak\n",
    "\t\t\tyield line\n",
    "\t\t\t\n",
    "def get_sent_len(txt):\n",
    "\ttokenized = tokenizer(txt,return_tensors=\"pt\",padding=True).to(device)\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "def get_emb_and_sent_chunk(batch_lines):\n",
    "\t\"\"\"\n",
    "\t\tgiven a chunk of sentences, return the predicted matrix of sentences in chunk\n",
    "\t\"\"\"\n",
    "\tbatch_inputs = tokenizer(batch_lines,return_tensors='pt',padding=True,truncation=True).to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\tbatch_outputs = model(**batch_inputs)\n",
    "\treturn batch_outputs[1]\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument(\"--local_rank\",type=int)\n",
    "\tparser.add_argument(\"--root-path\",help='root path for workspace')\n",
    "\tparser.add_argument(\"--threshold\",help='threshold for filtering, only select pairs which have higher values')\n",
    "\tparser.add_argument(\"--model-path',help='path for the pretrained model\")\n",
    "\tparser.add_argument(\"--src-lang',help='source lang\")\n",
    "\tparser.add_argument(\"--tgt-lang',help='target lang\")\n",
    "\tparser.add_argument(\"--indentifier\",help=\"identifier for wild-char matching. eg. article.01.sent-seg.zh, 01.sent-seg could be an identifier. file will be selected by *.identifier.lang\")\n",
    "\tparser.add_argument(\"--output-path\",help=\"path of the output lines\")\n",
    "\tparser.add_argument(\"--output-filter-path\",help=\"path of the output file\")\n",
    "\tparser.add_argument(\"--output-filter-path\",\"path of filted file\")\n",
    "\tparser.add_argument(\"--embed-only\",action=\"store_true\",help=\"only claculate embed of source senteces\")\n",
    "\tparser.add_argument(\"--f-content-prefix\",help='path of file content stored in a json file')\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\tfile_root_path  = args.root_path\n",
    "\tthreshold = float(args.threshold)\n",
    "\tmodel_path = args.model_path\n",
    "\ttokenizer = BertTokenizerFast.from_pretrained(model_path,model_max_length=512)\n",
    "\tmodel = BertModel.from_pretrained(model_path)\n",
    "\tdevice = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\tif torch.cuda.device_count() > 1:\n",
    "\t\tprint(\"using one cuda device\")\n",
    "\tmodel = model.to(device)\n",
    "\tsrc_lang, tgt_lang = args.src_lang, args.tgt_lang\n",
    "\n",
    "\tidentifier = args.identifier\n",
    "\tout_file = args.output_path\n",
    "\tout_filter = args.output_filter_path\n",
    "\tmodel = model.eval()\n",
    "\n",
    "\tstart = datetime.now()\n",
    "\twith open(f\"{file_root_path}/{out_file}\",\"w\") as aligned_file, open(\"{file_root_path}/{out_filter}\",\"w\") as f_filt:\n",
    "\t\tn_total_tgt_lines = 0\n",
    "\t\tn_matched_tgt_lines = 0\n",
    "\n",
    "\t\tsrc_lang_files = sorted(glob.glob(os.path.join(file_root_path,f'*.{identifier}.{src_lang}')))\n",
    "\t\ttgt_lang_files = sorted(glob.glob(os.path.join(file_root_path,f'*.{identifier}.{tgt_lang}')))\n",
    "\t\tassert len(src_lang_files) == len(tgt_lang_files)\n",
    "\n",
    "\tfor src_file, tgt_file in zip(src_lang_files,tgt_lang_files):\n",
    "\t\tfilename = os.path.basename(src_file)[:-3]\n",
    "\t\ttry:\n",
    "\t\t\twith open(f\"{args.f_content_prefix}.{src_lang}\") as f_s , open(f\"{args.f_content_prefix}.{tgt_lang}\") as f_t:\n",
    "\t\t\t\tsrc_lines = js.load(f_s)\n",
    "\t\t\t\ttgt_lines = js.load(f_t)\n",
    "\t\t\t\tprint(\"load json file\")\n",
    "\t\texcept:\n",
    "\t\t\tsrc_lines  = [x.strip() for x in read_in_chunks(src_file)]\n",
    "\t\t\ttgt_lines  = [x.strip() for x in read_in_chunks(tgt_file)]\n",
    "\t\t\tif args.f_content_prefix:\n",
    "\t\t\t\twith open(f\"{args.f_content_prefix}.{src_lang}\") as f_s , open(f\"{args.f_content_prefix}.{tgt_lang}\") as f_t:\n",
    "\t\t\t\t\tjs.dump(src_lines,f_s)\n",
    "\t\t\t\t\tjs.dump(tgt_lines,f_t)\n",
    "\t\t\t\t\tprint(\"stored file content to a json file\")\n",
    "\t\tstep = 100\n",
    "\t\tfor i in range(0,len(src_lines),step):\n",
    "\t\t\tsrc_batch_embeddings, tgt_batch_embeddings = [],[]\n",
    "\t\t\tsrc_batch_sents = src_lines[i:i+step]\n",
    "\t\t\ttgt_batch_sents = tgt_lines[i:i+step]\n",
    "\t\t\tassert len(src_batch_sents) == len(tgt_batch_sents)\n",
    "\n",
    "\t\t\tsrc_batch_embeddings = get_emb_and_sent_chunk(src_batch_sents)\n",
    "\t\t\tsrc_batch_embeddings = F.normalize(src_batch_embeddings,p=2)\n",
    "\t\t\tbatch_right = step if i+ step <= len(src_lines) else len(src_lines) -i\n",
    "\n",
    "\t\t\tif args.emb_only:\n",
    "\t\t\t\tsrc_batch_embeddings = src_batch_embeddings.tolist()\n",
    "\t\t\t\tfor idx in range(batch_right):\n",
    "\t\t\t\t\temb = ' '.join([str(x) for x in src_batch_embeddings[idx]])\n",
    "\t\t\t\t\taligned_file.write(f\"{src_batch_sents[idx]}\\t{filename}\\t{i+idx+1}\\t{emb}\\n\")\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ttgt_batch_embeddings = get_emb_and_sent_chunk(tgt_batch_sents)\n",
    "\t\t\ttgt_batch_embeddings = F.normalize(tgt_batch_embeddings,p=2)\n",
    "\t\t\tn_total_tgt_lines +=len(src_batch_sents)\n",
    "\n",
    "\t\t\tbatch_scores = torch.matmul(tgt_batch_embeddings,src_batch_embeddings.transpose(0,1))\n",
    "\t\t\tfor idx in range(batch_right):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tscore = batch_scores[idx][idx].item()\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(\"index %d of file %s : \"%(idx,filename))\n",
    "\t\t\t\t\tprint(\"{}\\t{}\".format(src_batch_sents[idx],tgt_batch_sents[idx]))\n",
    "\t\t\t\t\tprint(\"shape of matrix:\", batch_scores.shape)\n",
    "\t\t\t\tif score > threshold:\n",
    "\t\t\t\t\taligned_file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(score,src_batch_sents[idx],tgt_batch_sents[idx],filename,i+idx+1))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf_filt.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(score,src_batch_sents[idx],tgt_batch_sents[idx],filename,i+idx+1))\n",
    "\t\t\n",
    "\tend = datetime.now()\n",
    "\tseconds = (end-start).seconds\n",
    "\tprint(\"f{len(src_lang_files)} file processed, took {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## score.sh 脚本\n",
    "## 需要在$PATH/data目录下放两个文件，一个是源端句子en-zh.en ,一个是目标端句子en-zh.zh，会生成一个en-zh.out文件\n",
    "\n",
    "\n",
    "PATH=~/score_Labse\n",
    "cd $PATH/data\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python $PATH/alignment_extractor.py\\\n",
    "--root-path .\\\n",
    "--threshold 0 \\\n",
    "--model-path $path/Labse\\\n",
    "--src-lang en \\\n",
    "--tgt-lang zh \\\n",
    "--identifier en-zh \\\n",
    "--output-path en-zh.out \\\n",
    "--output-filter-path en-zh.filter \\\n",
    "--f-content-prefix en-zh.json \n",
    "\n",
    "rm -rf $PATH/data/en-zh.json*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
